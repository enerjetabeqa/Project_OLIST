{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a4e0701-cb83-4261-9b7b-ce3d8bcef061",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8a78f92-8995-4083-897d-2af6281c3ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "localhost master olist_user olist2025\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from create_db_connection_strings import create_connection_string, create_sql_alchemy_engine\n",
    "from read_env_file import SRVR, DTBS, USRNM, PSSWRD\n",
    "from execute_sql import execute_sql_statement\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "594346a5-caa6-4b0a-a0a8-101463d75c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\Downloads\\Project_OLIST - Copy\\NOTEBOOKS\n"
     ]
    }
   ],
   "source": [
    "# Current path\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ee0b3f5-c3f1-4d44-afe2-3c8dc6164c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path of the dataset\n",
    "csv_path = \"../OLIST_DATASET/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9aed62b-a5e6-496c-82cb-c1abeff47bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all of the csv file names\n",
    "print(os.listdir(csv_path)) \n",
    "all_csv = os.listdir(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67902d24-2ce8-40d4-a189-35c54bcae5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Data Cleaning and Transformation\n",
    "\n",
    "# 4.1.1 Load all CSV files into a Jupyter Notebook using Pandas\n",
    "# 4.1.2 Clean the datasets:\n",
    "# 4.1.2.2 Remove any duplicate rows\n",
    "\n",
    "# This method reads the csv files and also drops the duplicates\n",
    "def pandas_read_unique_csv(base_csv_path: str, file: str):\n",
    "    print(f\"The file path is: {base_csv_path}{file}\")\n",
    "    df = pd.read_csv(f\"{base_csv_path}{file}\")\n",
    "    df_no_duplicates = df.drop_duplicates()\n",
    "    return df_no_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1e1c6c7-ae30-45fd-8300-6225705d851c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method programatically creates variables and assigns them the dataframe created from a csv file\n",
    "def create_dataframes_programatically(variable: str,value: str,base_path: str):\n",
    "    var_name = variable\n",
    "    globals()[var_name] = pandas_read_unique_csv(base_path,value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc6a134-c6e4-4591-a5c2-34afc2e7aa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This iterates through all of the csv files, creates the variables and the respective dataframes\n",
    "for csv_file in all_csv:\n",
    "    dtfrm = f\"df_{csv_file.split('.')[0]}\"\n",
    "    print(f\"The variable to be created is: {dtfrm}\")\n",
    "    create_dataframes_programatically(dtfrm,csv_file,csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9db335f-bc2e-4123-8c05-4bc37dbae569",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for csv_file in all_csv:\n",
    "    var_name = f\"df_{csv_file.split('.')[0]}\"\n",
    "    print(f\"\\nHead of: {var_name}\")\n",
    "    print(globals()[var_name].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ad138f8-9594-4e63-bf00-657f71fa528a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates a connection string based on the enviroment variables\n",
    "arg_conn = create_connection_string(SRVR,DTBS,USRNM,PSSWRD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edb97eaf-3a7d-429a-b892-ed776ca29ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates the sqlalchemy engine from the connection\n",
    "arg_eng = create_sql_alchemy_engine(arg_conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f828efcc-881c-4d65-a3b1-3c8f88b118f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_stat = \"CREATE DATABASE OLIST;\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e074bcb-a43e-4113-b285-9a40adc4094e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = execute_sql_statement(arg_eng,sql_stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0af0bf8-fd51-4895-8ab6-4ced920adc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "arg_conn_olist = create_connection_string(SRVR,\"OLIST\",USRNM,PSSWRD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94dc2c15-9edb-42b1-9237-e03fd7658b77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "arg_eng_olist = create_sql_alchemy_engine(arg_conn_olist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9324f942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Data Cleaning and Transformation\n",
    "\n",
    "# 4.1.1 Load all CSV files into a Jupyter Notebook using Pandas\n",
    "# 4.1.2 Clean the datasets:\n",
    "# 4.1.2.1 Handle missing values by filling them with appropriate data or dropping rows/columns.\n",
    "\n",
    "# olist_products\n",
    "### fill null values\n",
    "df_olist_products_dataset['product_category_name'].fillna('desconhecida', inplace=True)\n",
    "\n",
    "df_olist_products_dataset['product_name_lenght'] = df_olist_products_dataset['product_name_lenght'].fillna(-1)\n",
    "\n",
    "df_olist_products_dataset['product_description_lenght'] = df_olist_products_dataset['product_description_lenght'].fillna(-1)\n",
    "\n",
    "df_olist_products_dataset['product_photos_qty'] = df_olist_products_dataset['product_description_lenght'].fillna(-1)\n",
    "\n",
    "df_olist_products_dataset.to_sql(\"olist_products\", con=arg_eng_olist, if_exists=\"replace\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ecf52e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "475"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# olist_sellers\n",
    "df_olist_sellers_dataset.to_sql(\"olist_sellers\", con=arg_eng_olist, if_exists=\"replace\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e435bfea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# product_category_name_translation\n",
    "# Add a new category that was used to fill null values\n",
    "df_product_category_name_translation.loc[len(df_product_category_name_translation)] = ['desconhecida', 'unknown']\n",
    "\n",
    "df_product_category_name_translation.to_sql(\"product_category_name_translation\", con=arg_eng_olist, if_exists=\"replace\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53c13911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# olist_orders\n",
    "\n",
    "# Fill Null values\n",
    "df_olist_orders_dataset['order_delivered_carrier_date'] = df_olist_orders_dataset['order_delivered_carrier_date'].fillna('1900-01-01 00:00:00')\n",
    "df_olist_orders_dataset['order_delivered_customer_date'] = df_olist_orders_dataset['order_delivered_customer_date'].fillna('1900-01-01 00:00:00')\n",
    "\n",
    "# Convert columns to datetime\n",
    "df_olist_orders_dataset['order_estimated_delivery_date'] = pd.to_datetime(df_olist_orders_dataset['order_estimated_delivery_date'])\n",
    "df_olist_orders_dataset['order_purchase_timestamp'] = pd.to_datetime(df_olist_orders_dataset['order_purchase_timestamp'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729d36d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 Creating Calculated Columns\n",
    "# 4.2.1 Create the following calculated columns in the order items dataset:\n",
    "# 4.2.1.1 Total Price: Sum of product price and freight value\n",
    "\n",
    "# olist_order_items\n",
    "\n",
    "# Total Price: Sum of product price and freight value.\n",
    "df_olist_order_items_dataset['total_price'] = df_olist_order_items_dataset['price'] + df_olist_order_items_dataset['freight_value']\n",
    "\n",
    "#4.2.1.2 Delivery Time: Difference between the delivery date and the order purchase date. \n",
    "\n",
    "# olist_orders\n",
    "\n",
    "# Delivery Time: Difference between the delivery date and the order purchase date.\n",
    "df_olist_orders_dataset['delivery_time'] = df_olist_orders_dataset['order_estimated_delivery_date'] - df_olist_orders_dataset['order_purchase_timestamp']\n",
    "\n",
    "df_olist_orders_dataset['delivery_time'] = df_olist_orders_dataset['delivery_time'].astype(str)\n",
    "df_olist_orders_dataset.to_sql(\"olist_orders\", con=arg_eng_olist, if_exists=\"replace\", index=False)\n",
    "\n",
    "# 4.2.1.3 Payment Count: Sum of payment installments for each order. \n",
    "\n",
    "\n",
    "# olist_order_payments\n",
    "\n",
    "#Optional:\n",
    "# Drop all duplicate payment_count columns if they exist\n",
    "df_olist_order_payments_dataset = df_olist_order_payments_dataset.drop(\n",
    "    columns=[col for col in df_olist_order_payments_dataset.columns if col.startswith('payment_count')],\n",
    "    errors='ignore'\n",
    ")\n",
    "\n",
    "# Create the payment_count DataFrame\n",
    "payment_count_df = df_olist_order_payments_dataset.groupby('order_id', as_index=False).agg(\n",
    "    payment_count=('payment_installments', 'sum')\n",
    ")\n",
    "\n",
    "# Merge the new payment_count column\n",
    "df_olist_order_payments_dataset = df_olist_order_payments_dataset.merge(\n",
    "    payment_count_df, on='order_id', how='left'\n",
    ")\n",
    "\n",
    "#Save the updated dataframe to SQL\n",
    "df_olist_order_payments_dataset.to_sql(\"olist_order_payments\", con=arg_eng_olist, if_exists=\"replace\", index=False)\n",
    "\n",
    "\n",
    "# Display the result\n",
    "print(df_olist_order_payments_dataset.head())\n",
    "\n",
    "# 4.2.1.4 Profit Margin: Subtract freight value from product price to calculate a rough profit \n",
    "# estimate. \n",
    "\n",
    "# olist_order_items\n",
    "\n",
    "# Profit Margin: Subtract freight value from product price to calculate a rough profit estimate.\n",
    "\n",
    "df_olist_order_items_dataset['profit_margin'] = df_olist_order_items_dataset['price'] - df_olist_order_items_dataset['freight_value']\n",
    "\n",
    "df_olist_order_items_dataset.to_sql(\"olist_order_items\", con=arg_eng_olist, if_exists=\"replace\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd3ac1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# olist_customers\n",
    "\n",
    "df_olist_customers_dataset.to_sql(\"olist_customers\", con=arg_eng_olist, if_exists=\"replace\", index=False)\n",
    "\n",
    "# olist_geolocation\n",
    "\n",
    "df_olist_geolocation_dataset.to_sql(\"olist_geolocation\", con=arg_eng_olist, if_exists=\"replace\", index=False)\n",
    "\n",
    "# olist_order_reviews\n",
    "\n",
    "df_olist_order_reviews_dataset.to_sql(\"olist_order_reviews\", con=arg_eng_olist, if_exists=\"replace\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c28773c-5c81-4a04-8fd5-301ba58574d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
